{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.20.30:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x28d6e8f2350>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pickle\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/C:/Users/LSouq/OneDrive/Documents/GitHub/ludovic.souquet.formation/pyspark/mushrooms.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_pyspark \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mLSouq\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mOneDrive\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDocuments\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mGitHub\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mludovic.souquet.formation\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mpyspark\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mmushrooms.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LSouq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[1;32mc:\\Users\\LSouq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\LSouq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/C:/Users/LSouq/OneDrive/Documents/GitHub/ludovic.souquet.formation/pyspark/mushrooms.csv."
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv(r\"C:\\Users\\LSouq\\OneDrive\\Documents\\GitHub\\ludovic.souquet.formation\\pyspark\\mushrooms.csv\",inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----------+---------+-------+----+---------------+------------+---------+----------+-----------+----------+------------------------+------------------------+----------------------+----------------------+---------+----------+-----------+---------+-----------------+----------+-------+\n",
      "|class|cap-shape|cap-surface|cap-color|bruises|odor|gill-attachment|gill-spacing|gill-size|gill-color|stalk-shape|stalk-root|stalk-surface-above-ring|stalk-surface-below-ring|stalk-color-above-ring|stalk-color-below-ring|veil-type|veil-color|ring-number|ring-type|spore-print-color|population|habitat|\n",
      "+-----+---------+-----------+---------+-------+----+---------------+------------+---------+----------+-----------+----------+------------------------+------------------------+----------------------+----------------------+---------+----------+-----------+---------+-----------------+----------+-------+\n",
      "|    p|        x|          s|        n|      t|   p|              f|           c|        n|         k|          e|         e|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                k|         s|      u|\n",
      "|    e|        x|          s|        y|      t|   a|              f|           c|        b|         k|          e|         c|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                n|         n|      g|\n",
      "|    e|        b|          s|        w|      t|   l|              f|           c|        b|         n|          e|         c|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                n|         n|      m|\n",
      "|    p|        x|          y|        w|      t|   p|              f|           c|        n|         n|          e|         e|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                k|         s|      u|\n",
      "|    e|        x|          s|        g|      f|   n|              f|           w|        b|         k|          t|         e|                       s|                       s|                     w|                     w|        p|         w|          o|        e|                n|         a|      g|\n",
      "+-----+---------+-----------+---------+-------+----+---------------+------------+---------+----------+-----------+----------+------------------------+------------------------+----------------------+----------------------+---------+----------+-----------+---------+-----------------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- cap-shape: string (nullable = true)\n",
      " |-- cap-surface: string (nullable = true)\n",
      " |-- cap-color: string (nullable = true)\n",
      " |-- bruises: string (nullable = true)\n",
      " |-- odor: string (nullable = true)\n",
      " |-- gill-attachment: string (nullable = true)\n",
      " |-- gill-spacing: string (nullable = true)\n",
      " |-- gill-size: string (nullable = true)\n",
      " |-- gill-color: string (nullable = true)\n",
      " |-- stalk-shape: string (nullable = true)\n",
      " |-- stalk-root: string (nullable = true)\n",
      " |-- stalk-surface-above-ring: string (nullable = true)\n",
      " |-- stalk-surface-below-ring: string (nullable = true)\n",
      " |-- stalk-color-above-ring: string (nullable = true)\n",
      " |-- stalk-color-below-ring: string (nullable = true)\n",
      " |-- veil-type: string (nullable = true)\n",
      " |-- veil-color: string (nullable = true)\n",
      " |-- ring-number: string (nullable = true)\n",
      " |-- ring-type: string (nullable = true)\n",
      " |-- spore-print-color: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      " |-- habitat: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "categoricalColumns = [\n",
    "    'class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises',\n",
    "    'odor', 'gill-attachment', 'gill-spacing', 'gill-size',\n",
    "    'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
    "    'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring',\n",
    "    'veil-type', 'veil-color', 'ring-number', 'ring-type', \n",
    "    'spore-print-color', 'population', 'habitat'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number', 'ring-type', 'spore-print-color', 'population', 'habitat']\n"
     ]
    }
   ],
   "source": [
    "print(categoricalColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: 2 valeurs uniques\n",
      "cap-shape: 6 valeurs uniques\n",
      "cap-surface: 4 valeurs uniques\n",
      "cap-color: 10 valeurs uniques\n",
      "bruises: 2 valeurs uniques\n",
      "odor: 9 valeurs uniques\n",
      "gill-attachment: 2 valeurs uniques\n",
      "gill-spacing: 2 valeurs uniques\n",
      "gill-size: 2 valeurs uniques\n",
      "gill-color: 12 valeurs uniques\n",
      "stalk-shape: 2 valeurs uniques\n",
      "stalk-root: 5 valeurs uniques\n",
      "stalk-surface-above-ring: 4 valeurs uniques\n",
      "stalk-surface-below-ring: 4 valeurs uniques\n",
      "stalk-color-above-ring: 9 valeurs uniques\n",
      "stalk-color-below-ring: 9 valeurs uniques\n",
      "veil-type: 1 valeurs uniques\n",
      "veil-color: 4 valeurs uniques\n",
      "ring-number: 3 valeurs uniques\n",
      "ring-type: 5 valeurs uniques\n",
      "spore-print-color: 9 valeurs uniques\n",
      "population: 6 valeurs uniques\n",
      "habitat: 7 valeurs uniques\n"
     ]
    }
   ],
   "source": [
    "for col_name in categoricalColumns:\n",
    "    unique_values = df_pyspark.select(col_name).distinct().count()\n",
    "    print(f\"{col_name}: {unique_values} valeurs uniques\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol+\"_encoded\").fit(df_pyspark)\n",
    "    df_pyspark = stringIndexer.transform(df_pyspark)\n",
    "    df_pyspark = df_pyspark.withColumn(categoricalCol+\"_encoded\", df_pyspark[categoricalCol+\"_encoded\"].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des colonnes originales et encodées que vous souhaitez sélectionner\n",
    "encoded_df = df_pyspark.select(\n",
    "    \"class_encoded\",  # Remplacez par le nom de la colonne encodée que vous avez créée\n",
    "    \"cap-shape_encoded\", \n",
    "    \"cap-surface_encoded\",\n",
    "    \"cap-color_encoded\",\n",
    "    \"bruises_encoded\",\n",
    "    \"odor_encoded\",\n",
    "    \"gill-attachment_encoded\",\n",
    "    \"gill-spacing_encoded\",\n",
    "    \"gill-size_encoded\",\n",
    "    \"gill-color_encoded\",\n",
    "    \"stalk-shape_encoded\",\n",
    "    \"stalk-root_encoded\",\n",
    "    \"stalk-surface-above-ring_encoded\",\n",
    "    \"stalk-surface-below-ring_encoded\",\n",
    "    \"stalk-color-above-ring_encoded\",\n",
    "    \"stalk-color-below-ring_encoded\",\n",
    "    \"veil-type_encoded\",\n",
    "    \"veil-color_encoded\",\n",
    "    \"ring-number_encoded\",\n",
    "    \"ring-type_encoded\",\n",
    "    \"spore-print-color_encoded\",\n",
    "    \"population_encoded\",\n",
    "    \"habitat_encoded\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+-------------------+-----------------+---------------+------------+-----------------------+--------------------+-----------------+------------------+-------------------+------------------+--------------------------------+--------------------------------+------------------------------+------------------------------+-----------------+------------------+-------------------+-----------------+-------------------------+------------------+---------------+\n",
      "|class_encoded|cap-shape_encoded|cap-surface_encoded|cap-color_encoded|bruises_encoded|odor_encoded|gill-attachment_encoded|gill-spacing_encoded|gill-size_encoded|gill-color_encoded|stalk-shape_encoded|stalk-root_encoded|stalk-surface-above-ring_encoded|stalk-surface-below-ring_encoded|stalk-color-above-ring_encoded|stalk-color-below-ring_encoded|veil-type_encoded|veil-color_encoded|ring-number_encoded|ring-type_encoded|spore-print-color_encoded|population_encoded|habitat_encoded|\n",
      "+-------------+-----------------+-------------------+-----------------+---------------+------------+-----------------------+--------------------+-----------------+------------------+-------------------+------------------+--------------------------------+--------------------------------+------------------------------+------------------------------+-----------------+------------------+-------------------+-----------------+-------------------------+------------------+---------------+\n",
      "|            1|                0|                  1|                0|              1|           6|                      0|                   0|                1|                 7|                  1|                 2|                               0|                               0|                             0|                             0|                0|                 0|                  0|                0|                        2|                 2|              4|\n",
      "|            0|                0|                  1|                3|              1|           4|                      0|                   0|                0|                 7|                  1|                 3|                               0|                               0|                             0|                             0|                0|                 0|                  0|                0|                        1|                 3|              1|\n",
      "|            0|                3|                  1|                4|              1|           5|                      0|                   0|                0|                 3|                  1|                 3|                               0|                               0|                             0|                             0|                0|                 0|                  0|                0|                        1|                 3|              5|\n",
      "|            1|                0|                  0|                4|              1|           6|                      0|                   0|                1|                 3|                  1|                 2|                               0|                               0|                             0|                             0|                0|                 0|                  0|                0|                        2|                 2|              4|\n",
      "|            0|                0|                  1|                1|              0|           0|                      0|                   1|                0|                 7|                  0|                 2|                               0|                               0|                             0|                             0|                0|                 0|                  0|                1|                        1|                 4|              1|\n",
      "+-------------+-----------------+-------------------+-----------------+---------------+------------+-----------------------+--------------------+-----------------+------------------+-------------------+------------------+--------------------------------+--------------------------------+------------------------------+------------------------------+-----------------+------------------+-------------------+-----------------+-------------------------+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "featureAssembler = VectorAssembler(inputCols=[\n",
    "    \"class_encoded\",\n",
    "    \"cap-shape_encoded\", \n",
    "    \"cap-surface_encoded\",\n",
    "    \"cap-color_encoded\",\n",
    "    \"bruises_encoded\",\n",
    "    \"odor_encoded\",\n",
    "    \"gill-attachment_encoded\",\n",
    "    \"gill-spacing_encoded\",\n",
    "    \"gill-size_encoded\",\n",
    "    \"gill-color_encoded\",\n",
    "    \"stalk-shape_encoded\",\n",
    "    \"stalk-root_encoded\",\n",
    "    \"stalk-surface-above-ring_encoded\",\n",
    "    \"stalk-surface-below-ring_encoded\",\n",
    "    \"stalk-color-above-ring_encoded\",\n",
    "    \"stalk-color-below-ring_encoded\",\n",
    "    \"veil-type_encoded\",\n",
    "    \"veil-color_encoded\",\n",
    "    \"ring-number_encoded\",\n",
    "    \"ring-type_encoded\",\n",
    "    \"spore-print-color_encoded\",\n",
    "    \"population_encoded\",\n",
    "    \"habitat_encoded\"],outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = featureAssembler.transform(encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|            features|class_encoded|\n",
      "+--------------------+-------------+\n",
      "|(23,[0,2,4,5,8,9,...|            1|\n",
      "|(23,[2,3,4,5,9,10...|            0|\n",
      "|(23,[1,2,3,4,5,9,...|            0|\n",
      "|(23,[0,3,4,5,8,9,...|            1|\n",
      "|(23,[2,3,7,9,11,1...|            0|\n",
      "|(23,[3,4,5,9,10,1...|            0|\n",
      "|(23,[1,2,3,4,5,9,...|            0|\n",
      "|(23,[1,3,4,5,9,10...|            0|\n",
      "|(23,[0,3,4,5,8,9,...|            1|\n",
      "|(23,[1,2,3,4,5,9,...|            0|\n",
      "|(23,[3,4,5,9,10,1...|            0|\n",
      "|(23,[3,4,5,9,10,1...|            0|\n",
      "|(23,[1,2,3,4,5,9,...|            0|\n",
      "|(23,[0,3,4,5,8,9,...|            1|\n",
      "|(23,[2,7,9,11,13,...|            0|\n",
      "|(23,[1,2,3,8,9,10...|            0|\n",
      "|(23,[1,2,3,7,9,11...|            0|\n",
      "|(23,[0,2,4,5,8,9,...|            1|\n",
      "|(23,[0,3,4,5,8,9,...|            1|\n",
      "|(23,[0,2,4,5,8,9,...|            1|\n",
      "+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.select(\"features\",\"class_encoded\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = output.randomSplit([0.8, 0.2], seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'class_encoded', maxIter=10)\n",
    "\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+-------------------+-----------------+---------------+------------+-----------------------+--------------------+-----------------+------------------+-------------------+------------------+--------------------------------+--------------------------------+------------------------------+------------------------------+-----------------+------------------+-------------------+-----------------+-------------------------+------------------+---------------+--------------------+--------------------+--------------------+----------+\n",
      "|class_encoded|cap-shape_encoded|cap-surface_encoded|cap-color_encoded|bruises_encoded|odor_encoded|gill-attachment_encoded|gill-spacing_encoded|gill-size_encoded|gill-color_encoded|stalk-shape_encoded|stalk-root_encoded|stalk-surface-above-ring_encoded|stalk-surface-below-ring_encoded|stalk-color-above-ring_encoded|stalk-color-below-ring_encoded|veil-type_encoded|veil-color_encoded|ring-number_encoded|ring-type_encoded|spore-print-color_encoded|population_encoded|habitat_encoded|            features|       rawPrediction|         probability|prediction|\n",
      "+-------------+-----------------+-------------------+-----------------+---------------+------------+-----------------------+--------------------+-----------------+------------------+-------------------+------------------+--------------------------------+--------------------------------+------------------------------+------------------------------+-----------------+------------------+-------------------+-----------------+-------------------------+------------------+---------------+--------------------+--------------------+--------------------+----------+\n",
      "|            0|                0|                  0|                0|              0|           0|                      0|                   0|                0|                 2|                  1|                 0|                               3|                               3|                             3|                             3|                0|                 0|                  1|                0|                        0|                 1|              0|(23,[9,10,12,13,1...|[5.47473630140153...|[0.99582616030170...|       0.0|\n",
      "|            0|                0|                  0|                0|              1|           0|                      0|                   0|                0|                 1|                  0|                 0|                               0|                               0|                             0|                             0|                0|                 0|                  0|                0|                        1|                 0|              0|(23,[4,9,20],[1.0...|[8.08332897588418...|[0.9996914533684,...|       0.0|\n",
      "|            0|                0|                  0|                0|              1|           0|                      0|                   0|                0|                 1|                  0|                 0|                               0|                               0|                             0|                             1|                0|                 0|                  0|                0|                        1|                 1|              0|(23,[4,9,15,20,21...|[8.21586327413781...|[0.99972974201864...|       0.0|\n",
      "|            0|                0|                  0|                0|              1|           0|                      0|                   0|                0|                 1|                  0|                 0|                               0|                               0|                             1|                             0|                0|                 0|                  0|                0|                        2|                 0|              0|(23,[4,9,14,20],[...|[7.96218803696994...|[0.99965173134060...|       0.0|\n",
      "|            0|                0|                  0|                0|              1|           0|                      0|                   0|                0|                 1|                  0|                 0|                               0|                               0|                             1|                             0|                0|                 0|                  0|                0|                        2|                 1|              0|(23,[4,9,14,20,21...|[8.02740567453741...|[0.99967371261477...|       0.0|\n",
      "|            0|                0|                  0|                0|              1|           0|                      0|                   0|                0|                 1|                  0|                 0|                               0|                               0|                             1|                             2|                0|                 0|                  0|                0|                        2|                 0|              0|(23,[4,9,14,15,20...|[8.09682135834226...|[0.99969558718008...|       0.0|\n",
      "|            0|                0|                  0|                0|              1|           0|                      0|                   0|                0|                 1|                  0|                 0|                               0|                               0|                             2|                             2|                0|                 0|                  0|                0|                        1|                 1|              0|(23,[4,9,14,15,20...|[8.31504933403169...|[0.99975525509866...|       0.0|\n",
      "|            0|                0|                  0|                0|              1|           0|                      0|                   0|                0|                 1|                  0|                 0|                               0|                               0|                             2|                             2|                0|                 0|                  0|                0|                        2|                 0|              0|(23,[4,9,14,15,20...|[8.11275605794611...|[0.99970039802218...|       0.0|\n",
      "|            0|                0|                  0|                0|              1|           0|                      0|                   0|                0|                 2|                  0|                 0|                               0|                               0|                             0|                             0|                0|                 0|                  0|                0|                        1|                 1|              0|(23,[4,9,20,21],[...|[8.18733843442526...|[0.99972192412475...|       0.0|\n",
      "|            0|                0|                  0|                0|              1|           0|                      0|                   0|                0|                 2|                  0|                 0|                               0|                               0|                             0|                             0|                0|                 0|                  0|                0|                        2|                 0|              0|(23,[4,9,20],[1.0...|[7.98504515833968...|[0.99965959879369...|       0.0|\n",
      "|            0|                0|                  0|                0|              1|           0|                      0|                   0|                0|                 2|                  0|                 0|                               0|                               0|                             0|                             0|                0|                 0|                  0|                0|                        2|                 1|              0|(23,[4,9,20,21],[...|[8.05026279590715...|[0.99968108366525...|       0.0|\n",
      "|            0|                0|                  0|                0|              1|           0|                      0|                   0|                0|                 2|                  0|                 0|                               0|                               0|                             0|                             1|                0|                 0|                  0|                0|                        2|                 1|              0|(23,[4,9,15,20,21...|[8.11757945659331...|[0.99970183921257...|       0.0|\n",
      "|            0|                0|                  0|                0|              1|           0|                      0|                   0|                0|                 2|                  0|                 0|                               0|                               0|                             0|                             2|                0|                 0|                  0|                0|                        1|                 0|              0|(23,[4,9,15,20],[...|[8.25675411823011...|[0.99974056739095...|       0.0|\n",
      "|            0|                0|                  0|                0|              1|           0|                      0|                   0|                0|                 2|                  0|                 0|                               0|                               0|                             0|                             2|                0|                 0|                  0|                0|                        1|                 1|              0|(23,[4,9,15,20,21...|[8.32197175579758...|[0.99975694306515...|       0.0|\n",
      "|            0|                0|                  0|                0|              1|           0|                      0|                   0|                0|                 2|                  0|                 0|                               0|                               0|                             0|                             2|                0|                 0|                  0|                0|                        2|                 0|              0|(23,[4,9,15,20],[...|[8.11967847971201...|[0.99970246421656...|       0.0|\n",
      "|            0|                0|                  0|                0|              1|           0|                      0|                   0|                0|                 2|                  0|                 0|                               0|                               0|                             1|                             1|                0|                 0|                  0|                0|                        1|                 1|              0|(23,[4,9,14,15,20...|[8.27058979471527...|[0.99974413118756...|       0.0|\n",
      "|            0|                0|                  0|                0|              1|           0|                      0|                   0|                0|                 2|                  0|                 0|                               0|                               0|                             1|                             1|                0|                 0|                  0|                0|                        2|                 1|              0|(23,[4,9,14,15,20...|[8.13351415619717...|[0.99970655127856...|       0.0|\n",
      "|            0|                0|                  0|                0|              1|           0|                      0|                   0|                0|                 2|                  0|                 0|                               0|                               0|                             1|                             2|                0|                 0|                  0|                0|                        1|                 1|              0|(23,[4,9,14,15,20...|[8.33790645540143...|[0.99976078449063...|       0.0|\n",
      "|            0|                0|                  0|                0|              1|           0|                      0|                   0|                0|                 2|                  0|                 0|                               0|                               0|                             2|                             2|                0|                 0|                  0|                0|                        1|                 1|              0|(23,[4,9,14,15,20...|[8.35384115500529...|[0.99976456521810...|       0.0|\n",
      "|            0|                0|                  0|                0|              1|           0|                      0|                   0|                0|                 2|                  0|                 0|                               0|                               0|                             2|                             2|                0|                 0|                  0|                0|                        2|                 0|              0|(23,[4,9,14,15,20...|[8.15154787891972...|[0.99971179430849...|       0.0|\n",
      "+-------------+-----------------+-------------------+-----------------+---------------+------------+-----------------------+--------------------+-----------------+------------------+-------------------+------------------+--------------------------------+--------------------------------+------------------------------+------------------------------+-----------------+------------------+-------------------+-----------------+-------------------------+------------------+---------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "\n",
    "evaluator.setLabelCol(\"class_encoded\")\n",
    "\n",
    "evaluator.setPredictionCol(\"prediction\")\n",
    "\n",
    "evaluator.evaluate(predictions)\n",
    "\n",
    "print(evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC:  1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#Training Model\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'class_encoded', maxDepth = 3)\n",
    "dtModel = dt.fit(train)\n",
    "\n",
    "#Prediction\n",
    "predictions = dtModel.transform(test)\n",
    "#Evaluating the performance\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "evaluator.setLabelCol(\"class_encoded\")\n",
    "evaluator.setPredictionCol(\"prediction\")\n",
    "print(\"Test Area Under ROC: \",evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC:  1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#Training Model\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'class_encoded', numTrees = 500, maxDepth = 10)\n",
    "rfModel = rf.fit(train)\n",
    "#Prediction\n",
    "predictions = rfModel.transform(test)\n",
    "#Evaluating the performance\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "evaluator.setLabelCol(\"class_encoded\")\n",
    "evaluator.setPredictionCol(\"prediction\")\n",
    "print(\"Test Area Under ROC: \",evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('class', 'string'),\n",
       " ('cap-shape', 'string'),\n",
       " ('cap-surface', 'string'),\n",
       " ('cap-color', 'string'),\n",
       " ('bruises', 'string'),\n",
       " ('odor', 'string'),\n",
       " ('gill-attachment', 'string'),\n",
       " ('gill-spacing', 'string'),\n",
       " ('gill-size', 'string'),\n",
       " ('gill-color', 'string'),\n",
       " ('stalk-shape', 'string'),\n",
       " ('stalk-root', 'string'),\n",
       " ('stalk-surface-above-ring', 'string'),\n",
       " ('stalk-surface-below-ring', 'string'),\n",
       " ('stalk-color-above-ring', 'string'),\n",
       " ('stalk-color-below-ring', 'string'),\n",
       " ('veil-type', 'string'),\n",
       " ('veil-color', 'string'),\n",
       " ('ring-number', 'string'),\n",
       " ('ring-type', 'string'),\n",
       " ('spore-print-color', 'string'),\n",
       " ('population', 'string'),\n",
       " ('habitat', 'string'),\n",
       " ('class_encoded', 'int'),\n",
       " ('cap-shape_encoded', 'int'),\n",
       " ('cap-surface_encoded', 'int'),\n",
       " ('cap-color_encoded', 'int'),\n",
       " ('bruises_encoded', 'int'),\n",
       " ('odor_encoded', 'int'),\n",
       " ('gill-attachment_encoded', 'int'),\n",
       " ('gill-spacing_encoded', 'int'),\n",
       " ('gill-size_encoded', 'int'),\n",
       " ('gill-color_encoded', 'int'),\n",
       " ('stalk-shape_encoded', 'int'),\n",
       " ('stalk-root_encoded', 'int'),\n",
       " ('stalk-surface-above-ring_encoded', 'int'),\n",
       " ('stalk-surface-below-ring_encoded', 'int'),\n",
       " ('stalk-color-above-ring_encoded', 'int'),\n",
       " ('stalk-color-below-ring_encoded', 'int'),\n",
       " ('veil-type_encoded', 'int'),\n",
       " ('veil-color_encoded', 'int'),\n",
       " ('ring-number_encoded', 'int'),\n",
       " ('ring-type_encoded', 'int'),\n",
       " ('spore-print-color_encoded', 'int'),\n",
       " ('population_encoded', 'int'),\n",
       " ('habitat_encoded', 'int')]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class_encoded: long (nullable = true)\n",
      " |-- cap-shape_encoded: long (nullable = true)\n",
      " |-- cap-surface_encoded: long (nullable = true)\n",
      " |-- cap-color_encoded: long (nullable = true)\n",
      " |-- bruises_encoded: long (nullable = true)\n",
      " |-- odor_encoded: long (nullable = true)\n",
      " |-- gill-attachment_encoded: long (nullable = true)\n",
      " |-- gill-spacing_encoded: long (nullable = true)\n",
      " |-- gill-size_encoded: long (nullable = true)\n",
      " |-- gill-color_encoded: long (nullable = true)\n",
      " |-- stalk-shape_encoded: long (nullable = true)\n",
      " |-- stalk-root_encoded: long (nullable = true)\n",
      " |-- stalk-surface-above-ring_encoded: long (nullable = true)\n",
      " |-- stalk-surface-below-ring_encoded: long (nullable = true)\n",
      " |-- stalk-color-above-ring_encoded: long (nullable = true)\n",
      " |-- stalk-color-below-ring_encoded: long (nullable = true)\n",
      " |-- veil-type_encoded: long (nullable = true)\n",
      " |-- veil-color_encoded: long (nullable = true)\n",
      " |-- ring-number_encoded: long (nullable = true)\n",
      " |-- ring-type_encoded: long (nullable = true)\n",
      " |-- spore-print-color_encoded: long (nullable = true)\n",
      " |-- population_encoded: long (nullable = true)\n",
      " |-- habitat_encoded: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = rfModel.transform(test_features)\n",
    "predictions.printSchema()  # Doit afficher 'prediction' comme colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----------+---------+-------+----+---------------+------------+---------+----------+-----------+----------+------------------------+------------------------+----------------------+----------------------+---------+----------+-----------+---------+-----------------+----------+-------+\n",
      "|class|cap-shape|cap-surface|cap-color|bruises|odor|gill-attachment|gill-spacing|gill-size|gill-color|stalk-shape|stalk-root|stalk-surface-above-ring|stalk-surface-below-ring|stalk-color-above-ring|stalk-color-below-ring|veil-type|veil-color|ring-number|ring-type|spore-print-color|population|habitat|\n",
      "+-----+---------+-----------+---------+-------+----+---------------+------------+---------+----------+-----------+----------+------------------------+------------------------+----------------------+----------------------+---------+----------+-----------+---------+-----------------+----------+-------+\n",
      "|    p|        x|          s|        n|      t|   p|              f|           c|        n|         k|          e|         e|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                k|         s|      u|\n",
      "|    e|        x|          s|        y|      t|   a|              f|           c|        b|         k|          e|         c|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                n|         n|      g|\n",
      "|    e|        b|          s|        w|      t|   l|              f|           c|        b|         n|          e|         c|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                n|         n|      m|\n",
      "|    p|        x|          y|        w|      t|   p|              f|           c|        n|         n|          e|         e|                       s|                       s|                     w|                     w|        p|         w|          o|        p|                k|         s|      u|\n",
      "|    e|        x|          s|        g|      f|   n|              f|           w|        b|         k|          t|         e|                       s|                       s|                     w|                     w|        p|         w|          o|        e|                n|         a|      g|\n",
      "+-----+---------+-----------+---------+-------+----+---------------+------------+---------+----------+-----------+----------+------------------------+------------------------+----------------------+----------------------+---------+----------+-----------+---------+-----------------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- cap-shape: string (nullable = true)\n",
      " |-- cap-surface: string (nullable = true)\n",
      " |-- cap-color: string (nullable = true)\n",
      " |-- bruises: string (nullable = true)\n",
      " |-- odor: string (nullable = true)\n",
      " |-- gill-attachment: string (nullable = true)\n",
      " |-- gill-spacing: string (nullable = true)\n",
      " |-- gill-size: string (nullable = true)\n",
      " |-- gill-color: string (nullable = true)\n",
      " |-- stalk-shape: string (nullable = true)\n",
      " |-- stalk-root: string (nullable = true)\n",
      " |-- stalk-surface-above-ring: string (nullable = true)\n",
      " |-- stalk-surface-below-ring: string (nullable = true)\n",
      " |-- stalk-color-above-ring: string (nullable = true)\n",
      " |-- stalk-color-below-ring: string (nullable = true)\n",
      " |-- veil-type: string (nullable = true)\n",
      " |-- veil-color: string (nullable = true)\n",
      " |-- ring-number: string (nullable = true)\n",
      " |-- ring-type: string (nullable = true)\n",
      " |-- spore-print-color: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      " |-- habitat: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- cap-shape: string (nullable = true)\n",
      " |-- cap-surface: string (nullable = true)\n",
      " |-- cap-color: string (nullable = true)\n",
      " |-- bruises: string (nullable = true)\n",
      " |-- odor: string (nullable = true)\n",
      " |-- gill-attachment: string (nullable = true)\n",
      " |-- gill-spacing: string (nullable = true)\n",
      " |-- gill-size: string (nullable = true)\n",
      " |-- gill-color: string (nullable = true)\n",
      " |-- stalk-shape: string (nullable = true)\n",
      " |-- stalk-root: string (nullable = true)\n",
      " |-- stalk-surface-above-ring: string (nullable = true)\n",
      " |-- stalk-surface-below-ring: string (nullable = true)\n",
      " |-- stalk-color-above-ring: string (nullable = true)\n",
      " |-- stalk-color-below-ring: string (nullable = true)\n",
      " |-- veil-type: string (nullable = true)\n",
      " |-- veil-color: string (nullable = true)\n",
      " |-- ring-number: string (nullable = true)\n",
      " |-- ring-type: string (nullable = true)\n",
      " |-- spore-print-color: string (nullable = true)\n",
      " |-- population: string (nullable = true)\n",
      " |-- habitat: string (nullable = true)\n",
      " |-- class_encoded: double (nullable = false)\n",
      " |-- cap-shape_encoded: double (nullable = false)\n",
      " |-- cap-surface_encoded: double (nullable = false)\n",
      " |-- cap-color_encoded: double (nullable = false)\n",
      " |-- bruises_encoded: double (nullable = false)\n",
      " |-- odor_encoded: double (nullable = false)\n",
      " |-- gill-attachment_encoded: double (nullable = false)\n",
      " |-- gill-spacing_encoded: double (nullable = false)\n",
      " |-- gill-size_encoded: double (nullable = false)\n",
      " |-- gill-color_encoded: double (nullable = false)\n",
      " |-- stalk-shape_encoded: double (nullable = false)\n",
      " |-- stalk-root_encoded: double (nullable = false)\n",
      " |-- stalk-surface-above-ring_encoded: double (nullable = false)\n",
      " |-- stalk-surface-below-ring_encoded: double (nullable = false)\n",
      " |-- stalk-color-above-ring_encoded: double (nullable = false)\n",
      " |-- stalk-color-below-ring_encoded: double (nullable = false)\n",
      " |-- veil-type_encoded: double (nullable = false)\n",
      " |-- veil-color_encoded: double (nullable = false)\n",
      " |-- ring-number_encoded: double (nullable = false)\n",
      " |-- ring-type_encoded: double (nullable = false)\n",
      " |-- spore-print-color_encoded: double (nullable = false)\n",
      " |-- population_encoded: double (nullable = false)\n",
      " |-- habitat_encoded: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n",
      "+--------------------+--------------------+----------+\n",
      "|            features|         probability|prediction|\n",
      "+--------------------+--------------------+----------+\n",
      "|(23,[1,2,3,7,9,10...|[0.99998478260869...|       0.0|\n",
      "|(23,[1,2,3,7,9,10...|[0.99998478260869...|       0.0|\n",
      "|(23,[1,2,3,7,9,10...|[0.99998478260869...|       0.0|\n",
      "|(23,[1,2,3,7,9,10...|[0.99998478260869...|       0.0|\n",
      "|(23,[1,2,3,7,9,10...|[0.99998478260869...|       0.0|\n",
      "+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Accuracy du modèle Random Forest: 1.00\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Création de la session Spark\n",
    "spark = SparkSession.builder.appName(\"Mushroom Classification\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# Chargement des données\n",
    "df_pyspark = spark.read.csv(r\"C:\\Users\\LSouq\\OneDrive\\Documents\\GitHub\\ludovic.souquet.formation\\pyspark\\mushrooms.csv\", \n",
    "                             inferSchema=True, header=True)\n",
    "\n",
    "# Afficher les premières lignes et le schéma\n",
    "df_pyspark.show(5)\n",
    "df_pyspark.printSchema()\n",
    "\n",
    "# Indexation des colonnes catégoriques\n",
    "categoricalColumns = [\n",
    "    'class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises',\n",
    "    'odor', 'gill-attachment', 'gill-spacing', 'gill-size',\n",
    "    'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
    "    'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring',\n",
    "    'veil-type', 'veil-color', 'ring-number', 'ring-type', \n",
    "    'spore-print-color', 'population', 'habitat'\n",
    "]\n",
    "\n",
    "# Indexation de chaque colonne\n",
    "for col_name in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol=col_name, outputCol=col_name + \"_encoded\").fit(df_pyspark)\n",
    "    df_pyspark = stringIndexer.transform(df_pyspark)\n",
    "\n",
    "# Assemblage des fonctionnalités\n",
    "feature_cols = [col + \"_encoded\" for col in categoricalColumns]\n",
    "featureAssembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "output = featureAssembler.transform(df_pyspark)\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "train, test = output.randomSplit([0.8, 0.2], seed=17)\n",
    "\n",
    "# Création et entraînement du modèle Random Forest\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='class_encoded', numTrees=500, maxDepth=10)\n",
    "rfModel = rf.fit(train)\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "predictions = rfModel.transform(test)\n",
    "\n",
    "# Affichage du schéma des prédictions pour vérifier que 'prediction' est bien là\n",
    "predictions.printSchema()\n",
    "\n",
    "# Sélectionner et afficher les résultats des prédictions\n",
    "predictions.select(\"features\", \"probability\", \"prediction\").show(5)\n",
    "\n",
    "# Évaluation des performances\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"class_encoded\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Accuracy du modèle Random Forest: {accuracy:.2f}\")\n",
    "\n",
    "# Arrêter la session Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[class_encoded: bigint, cap-shape_encoded: bigint, cap-surface_encoded: bigint, cap-color_encoded: bigint, bruises_encoded: bigint, odor_encoded: bigint, gill-attachment_encoded: bigint, gill-spacing_encoded: bigint, gill-size_encoded: bigint, gill-color_encoded: bigint, stalk-shape_encoded: bigint, stalk-root_encoded: bigint, stalk-surface-above-ring_encoded: bigint, stalk-surface-below-ring_encoded: bigint, stalk-color-above-ring_encoded: bigint, stalk-color-below-ring_encoded: bigint, veil-type_encoded: bigint, veil-color_encoded: bigint, ring-number_encoded: bigint, ring-type_encoded: bigint, spore-print-color_encoded: bigint, population_encoded: bigint, habitat_encoded: bigint]\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o26948.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (192.168.20.30 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.GeneratedMethodAccessor446.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[183], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m (test_df)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Affichez le DataFrame de test\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m \u001b[43mtest_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Assemblage des fonctionnalités\u001b[39;00m\n\u001b[0;32m     47\u001b[0m feature_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcap-shape_encoded\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcap-surface_encoded\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhabitat_encoded\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     70\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\LSouq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\LSouq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\LSouq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\LSouq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\LSouq\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o26948.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (192.168.20.30 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.GeneratedMethodAccessor446.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Créez une session Spark\n",
    "spark = SparkSession.builder.appName(\"Mushroom Classification\").getOrCreate()\n",
    "\n",
    "# Exemple de données pour un champignon\n",
    "test_data = [\n",
    "    Row(\n",
    "        **{\n",
    "            \"class_encoded\": 0,  # Pas nécessaire pour la prédiction\n",
    "            \"cap-shape_encoded\": 5,\n",
    "            \"cap-surface_encoded\": 2,\n",
    "            \"cap-color_encoded\": 4,\n",
    "            \"bruises_encoded\": 1,\n",
    "            \"odor_encoded\": 6,\n",
    "            \"gill-attachment_encoded\": 1,\n",
    "            \"gill-spacing_encoded\": 0,\n",
    "            \"gill-size_encoded\": 1,\n",
    "            \"gill-color_encoded\": 2,\n",
    "            \"stalk-shape_encoded\": 1,\n",
    "            \"stalk-root_encoded\": 3,\n",
    "            \"stalk-surface-above-ring_encoded\": 2,\n",
    "            \"stalk-surface-below-ring_encoded\": 2,\n",
    "            \"stalk-color-above-ring_encoded\": 1,\n",
    "            \"stalk-color-below-ring_encoded\": 1,\n",
    "            \"veil-type_encoded\": 0,\n",
    "            \"veil-color_encoded\": 2,\n",
    "            \"ring-number_encoded\": 1,\n",
    "            \"ring-type_encoded\": 4,\n",
    "            \"spore-print-color_encoded\": 3,\n",
    "            \"population_encoded\": 1,\n",
    "            \"habitat_encoded\": 2\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "# Créez un DataFrame à partir des données de test\n",
    "test_df = spark.createDataFrame(test_data)\n",
    "print (test_df)\n",
    "\n",
    "# Affichez le DataFrame de test\n",
    "test_df.show()\n",
    "\n",
    "# Assemblage des fonctionnalités\n",
    "feature_columns = [\n",
    "    \"cap-shape_encoded\",\n",
    "    \"cap-surface_encoded\",\n",
    "    \"cap-color_encoded\",\n",
    "    \"bruises_encoded\",\n",
    "    \"odor_encoded\",\n",
    "    \"gill-attachment_encoded\",\n",
    "    \"gill-spacing_encoded\",\n",
    "    \"gill-size_encoded\",\n",
    "    \"gill-color_encoded\",\n",
    "    \"stalk-shape_encoded\",\n",
    "    \"stalk-root_encoded\",\n",
    "    \"stalk-surface-above-ring_encoded\",\n",
    "    \"stalk-surface-below-ring_encoded\",\n",
    "    \"stalk-color-above-ring_encoded\",\n",
    "    \"stalk-color-below-ring_encoded\",\n",
    "    \"veil-type_encoded\",\n",
    "    \"veil-color_encoded\",\n",
    "    \"ring-number_encoded\",\n",
    "    \"ring-type_encoded\",\n",
    "    \"spore-print-color_encoded\",\n",
    "    \"population_encoded\",\n",
    "    \"habitat_encoded\"\n",
    "]\n",
    "\n",
    "# Créer le vecteur de caractéristiques\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "test_features = assembler.transform(test_df)\n",
    "\n",
    "# Faire des prédictions sur les nouvelles données\n",
    "predictions = rfModel.transform(test_features)\n",
    "\n",
    "# Afficher les résultats des prédictions\n",
    "predictions.select(\"features\", \"probability\", \"prediction\").show()\n",
    "\n",
    "# Arrêter la session Spark (si vous avez terminé)\n",
    "# spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
